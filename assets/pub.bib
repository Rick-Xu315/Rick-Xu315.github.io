@article{10096655,
  author={Xu, Ruize and Feng, Ruoxuan and Zhang, Shi-Xiong and Hu, Di},
  journal={<span style="color: #0088cc; font-style: normal">ICASSP 2023.</span>  2023 IEEE International Conference on Acoustics, Speech and Signal Processing}, 
  title={MMCosine: Multi-Modal Cosine Loss Towards Balanced Audio-Visual Fine-Grained Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  keywords={Measurement;Signal processing;Acoustics;Task analysis;Speech processing;Optimization},
  url_Paper={https://arxiv.org/abs/2303.05338.pdf},
  doi={10.1109/ICASSP49357.2023.10096655},
  abstract={Audio-visual learning helps to comprehensively understand the world by fusing practical information from multiple modalities. However, recent studies show that the imbalanced optimization of uni-modal encoders in a jointlearning model is a bottleneck to enhancing the model's performance. We further find that the up-to-date imbalancemitigating methods fail on some audio-visual fine-grained tasks, which have a higher demand for distinguishable feature distribution. Fueled by the success of cosine loss that builds hyperspherical feature spaces and achieves lower intraclass angular variability, this paper proposes Multi-Modal Cosine loss, MMCosine. It performs a modality-wise L2 normalization to features and weights towards balanced and better multi-modal fine-grained learning. We demonstrate that our method can alleviate the imbalanced optimization from the perspective of weight norm and fully exploit the discriminability of the cosine metric. Extensive experiments prove the effectiveness of our method and the versatility with advanced multi-modal fusion strategies and up-todate imbalance-mitigating methods. The project page is https://gewu-lab.github.io/MMCosine/.},
url_Supplementary={https://rick-xu315.github.io/assets/pdf/ICASSP23_Sup.pdf},
}

  @article{jiang-etal-2024-comclip,
    title = {{C}om{CLIP}: Training-Free Compositional Image and Text Matching},
    author = {Jiang, Kenan  and
      He, Xuehai  and
      Xu, Ruize  and
      Wang, Xin},
    journal = {<span style="color: #0088cc; font-style: normal">NAACL 2024.</span> Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
    month = {jun},
    year = {2024},
    address = {Mexico City, Mexico},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2024.naacl-long.370},
    doi = {10.18653/v1/2024.naacl-long.370},
    pages ={6639--6659},
    abstract = {Contrastive Language-Image Pretraining (CLIP) has demonstrated great zero-shot performance for matching images and text. However, it is still challenging to adapt vision-language pretrained models like CLIP to compositional image and text matching {---} a more challenging image and text matching task requiring the model{'}s understanding of compositional word concepts and visual components. Towards better compositional generalization in zero-shot image and text matching, in this paper, we study the problem from a causal perspective: the erroneous semantics of individual entities are essentially confounders that cause the matching failure. Therefore, we propose a novel training-free compositional CLIP model (ComCLIP). ComCLIP disentangles input images into subjects, objects, and action subimages and composes CLIP{'}s vision encoder and text encoder to perform evolving matching over compositional text embedding and subimage embeddings. In this way, ComCLIP can mitigate spurious correlations introduced by the pretrained CLIP models and dynamically evaluate the importance of each component. Experiments on four compositional image-text matching datasets: Winoground, VL-checklist, SVO, and ComVG, and two general image-text retrieval datasets: Flick30K, and MSCOCO demonstrate the effectiveness of our plug-and-play method, which boosts the zero-shot inference ability of CLIP, SLIP, and BLIP2 even without further training or fine-tuning. Our codes can be found at https://github.com/eric-ai-lab/ComCLIP.}
}

@article{tsai2024detection,
  title = {From Detection to Deception: Are AI-Generated Image Detectors Adversarially Robust?},
  author = {Tsai, Yun-Yun and Xu, Ruize and Mao, Chengzhi and Yang, Junfeng},
  journal = {<span style="color: #0088cc; font-style: normal">CVPR 2024 Responsible Generative AI Workshop.</span>},
  year = {2024},
  url_Paper = {https://drive.google.com/file/d/13-Z0OBPEVs4OizMGW-hKQHvienPQACGP/view},
  abstract={Generative models are revolutionizing industries by synthesizing high-quality images, yet they pose societal risks
as they are exploited at scale for generating disinformation, propaganda, scams, and phishing attacks. Recent work has
developed detectors with remarkable accuracy in identifying images generated by current models, but the robustness of
the detectors remains to be explored. This paper investigates the robustness of these detectors against adversarial perturbations designed to elude detection. We observe that an end-to-end adversarial attack on the entire detection pipeline is ineffective due to the long stochastic process of diffusion models. Instead, we create intermediate guidance for the attack at the model internals. Empirical results on both black box and white box attacks demonstrate the importance of  our proposed intermediate supervision when constructing the attack. Our results show that our approach can fool the detector, reducing the detection accuracy by up to 69 points in the black-box setting and 91 to 100 points in the white-box setting. In addition, our attack transfers well togenerated images from unknown models, including StyleGAN. Our work suggests that existing AI-generated image detectors are easily deceived by adversarial perturbations, highlighting the need for more robust detectors.}
}

@inproceedings{yu2025confitv2improvingresumejob,
  title={ConFit v2: Improving Resume-Job Matching using Hypothetical Resume Embedding and Runner-Up Hard-Negative Mining},
  author={Yu, Xiao * and Xu, Ruize * and Xue, Chengyuan * and Zhang, Jinzhong and Ma, Xu and Yu, Zhou},
  journal={<span style="color: #0088cc; font-style: normal">ACL 2025.</span> Findings of the Association for Computational Linguistics (ACL)},
  year={2025},
  bibbase_note={(* Equal contribution)},
  url_Paper = {https://arxiv.org/abs/2502.12361}
}