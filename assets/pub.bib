@INPROCEEDINGS{10096655,
  author={Xu, Ruize and Feng, Ruoxuan and Zhang, Shi-Xiong and Hu, Di},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={MMCosine: Multi-Modal Cosine Loss Towards Balanced Audio-Visual Fine-Grained Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  keywords={Measurement;Signal processing;Acoustics;Task analysis;Speech processing;Optimization},
  doi={10.1109/ICASSP49357.2023.10096655}}

  @inproceedings{jiang-etal-2024-comclip,
    title = "{C}om{CLIP}: Training-Free Compositional Image and Text Matching",
    author = "Jiang, Kenan  and
      He, Xuehai  and
      Xu, Ruize  and
      Wang, Xin",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.370",
    doi = "10.18653/v1/2024.naacl-long.370",
    pages = "6639--6659",
    abstract = "Contrastive Language-Image Pretraining (CLIP) has demonstrated great zero-shot performance for matching images and text. However, it is still challenging to adapt vision-language pretrained models like CLIP to compositional image and text matching {---} a more challenging image and text matching task requiring the model{'}s understanding of compositional word concepts and visual components. Towards better compositional generalization in zero-shot image and text matching, in this paper, we study the problem from a causal perspective: the erroneous semantics of individual entities are essentially confounders that cause the matching failure. Therefore, we propose a novel training-free compositional CLIP model (ComCLIP). ComCLIP disentangles input images into subjects, objects, and action subimages and composes CLIP{'}s vision encoder and text encoder to perform evolving matching over compositional text embedding and subimage embeddings. In this way, ComCLIP can mitigate spurious correlations introduced by the pretrained CLIP models and dynamically evaluate the importance of each component. Experiments on four compositional image-text matching datasets: Winoground, VL-checklist, SVO, and ComVG, and two general image-text retrieval datasets: Flick30K, and MSCOCO demonstrate the effectiveness of our plug-and-play method, which boosts the zero-shot inference ability of CLIP, SLIP, and BLIP2 even without further training or fine-tuning. Our codes can be found at https://github.com/eric-ai-lab/ComCLIP.",
}

@inproceedings{tsai2024detection,
  title = {From Detection to Deception: Are AI-Generated Image Detectors Adversarially Robust?},
  author = {Tsai, Yun-Yun and Xu, Ruize and Mao, Chengzhi and Yang, Junfeng},
  booktitle = {CVPR 2024 Responsible Generative AI Workshop},
  year = {2024},
  url_Paper = {https://drive.google.com/file/d/13-Z0OBPEVs4OizMGW-hKQHvienPQACGP/view}
}