@article{10096655,
  author={Xu, Ruize and Feng, Ruoxuan and Zhang, Shi-Xiong and Hu, Di},
  journal={<span style="color: #0088cc; font-style: normal">ICASSP 2023.</span>  2023 IEEE International Conference on Acoustics, Speech and Signal Processing}, 
  title={MMCosine: Multi-Modal Cosine Loss Towards Balanced Audio-Visual Fine-Grained Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  keywords={Measurement;Signal processing;Acoustics;Task analysis;Speech processing;Optimization},
  url_Paper={https://arxiv.org/abs/2303.05338.pdf},
  doi={10.1109/ICASSP49357.2023.10096655},
  abstract={Audio-visual learning helps to comprehensively understand the world by fusing practical information from multiple modalities. However, recent studies show that the imbalanced optimization of uni-modal encoders in a jointlearning model is a bottleneck to enhancing the model's performance. We further find that the up-to-date imbalancemitigating methods fail on some audio-visual fine-grained tasks, which have a higher demand for distinguishable feature distribution. Fueled by the success of cosine loss that builds hyperspherical feature spaces and achieves lower intraclass angular variability, this paper proposes Multi-Modal Cosine loss, MMCosine. It performs a modality-wise L2 normalization to features and weights towards balanced and better multi-modal fine-grained learning. We demonstrate that our method can alleviate the imbalanced optimization from the perspective of weight norm and fully exploit the discriminability of the cosine metric. Extensive experiments prove the effectiveness of our method and the versatility with advanced multi-modal fusion strategies and up-todate imbalance-mitigating methods. The project page is https://gewu-lab.github.io/MMCosine/.},
url_Supplementary={https://rick-xu315.github.io/assets/pdf/ICASSP23_Sup.pdf},
}
@article{yu2025dyna,
  title={Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents},
  author={Yu, Xiao and Peng, Baolin and Xu, Ruize and Galley, Michel and Cheng, Hao and Nath, Suman and Gao, Jianfeng and Yu, Zhou},
  journal={arXiv preprint arXiv:2506.00320},
  url_Paper={https://arxiv.org/abs/2506.00320},
  year={2025},
  abstract={Recent progress in reasoning with large language models (LLMs), such as DeepSeek-R1, demonstrates impressive capabilities in domains like mathematics and coding, by exhibiting complex cognitive behaviors such as verification, goal decomposition, and self-reflection. However, it is unclear what behavior is effective and what behavior is missing for long-horizon AI agents tasks. In this work, we propose Dyna-Think, a thinking framework that integrates planning with an internal world model with reasoning and acting to enhance AI agent performance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning (DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with Dyna-Think, DIT reconstructs the thinking process of R1 to focus on performing world model simulation relevant to the proposed (and planned) action, and trains the policy using this reconstructed data. To enhance Dyna-Think, DDT uses a two-stage training process to first improve the agent's world modeling ability via objectives such as state prediction or critique generation, and then improve the agent's action via policy training. We evaluate our methods on OSWorld, and demonstrate that Dyna-Think improves the agent's in-domain and out-of-domain performance, achieving similar best-of-n performance compared to R1 while generating 2x less tokens on average. Our extensive empirical studies reveal that 1) using critique generation for world model training is effective to improve policy performance; and 2) AI agents with better performance correlate with better world modeling abilities. We believe our results suggest a promising research direction to integrate world model simulation into AI agents to enhance their reasoning, planning, and acting capabilities.}
}
  @article{jiang-etal-2024-comclip,
    title = {{C}om{CLIP}: Training-Free Compositional Image and Text Matching},
    author = {Jiang, Kenan  and
      He, Xuehai  and
      Xu, Ruize  and
      Wang, Xin},
    journal = {<span style="color: #0088cc; font-style: normal">NAACL 2024.</span> Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
    month = {jun},
    year = {2024},
    address = {Mexico City, Mexico},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2024.naacl-long.370},
    doi = {10.18653/v1/2024.naacl-long.370},
    pages ={6639--6659},
    abstract = {Contrastive Language-Image Pretraining (CLIP) has demonstrated great zero-shot performance for matching images and text. However, it is still challenging to adapt vision-language pretrained models like CLIP to compositional image and text matching {---} a more challenging image and text matching task requiring the model{'}s understanding of compositional word concepts and visual components. Towards better compositional generalization in zero-shot image and text matching, in this paper, we study the problem from a causal perspective: the erroneous semantics of individual entities are essentially confounders that cause the matching failure. Therefore, we propose a novel training-free compositional CLIP model (ComCLIP). ComCLIP disentangles input images into subjects, objects, and action subimages and composes CLIP{'}s vision encoder and text encoder to perform evolving matching over compositional text embedding and subimage embeddings. In this way, ComCLIP can mitigate spurious correlations introduced by the pretrained CLIP models and dynamically evaluate the importance of each component. Experiments on four compositional image-text matching datasets: Winoground, VL-checklist, SVO, and ComVG, and two general image-text retrieval datasets: Flick30K, and MSCOCO demonstrate the effectiveness of our plug-and-play method, which boosts the zero-shot inference ability of CLIP, SLIP, and BLIP2 even without further training or fine-tuning. Our codes can be found at https://github.com/eric-ai-lab/ComCLIP.}
}

@article{tsai2024detection,
  title = {From Detection to Deception: Are AI-Generated Image Detectors Adversarially Robust?},
  author = {Tsai, Yun-Yun and Xu, Ruize and Mao, Chengzhi and Yang, Junfeng},
  journal = {<span style="color: #0088cc; font-style: normal">CVPR 2024 Responsible Generative AI Workshop.</span>},
  year = {2024},
  url_Paper = {https://drive.google.com/file/d/13-Z0OBPEVs4OizMGW-hKQHvienPQACGP/view},
  abstract={Generative models are revolutionizing industries by synthesizing high-quality images, yet they pose societal risks
as they are exploited at scale for generating disinformation, propaganda, scams, and phishing attacks. Recent work has
developed detectors with remarkable accuracy in identifying images generated by current models, but the robustness of
the detectors remains to be explored. This paper investigates the robustness of these detectors against adversarial perturbations designed to elude detection. We observe that an end-to-end adversarial attack on the entire detection pipeline is ineffective due to the long stochastic process of diffusion models. Instead, we create intermediate guidance for the attack at the model internals. Empirical results on both black box and white box attacks demonstrate the importance of  our proposed intermediate supervision when constructing the attack. Our results show that our approach can fool the detector, reducing the detection accuracy by up to 69 points in the black-box setting and 91 to 100 points in the white-box setting. In addition, our attack transfers well togenerated images from unknown models, including StyleGAN. Our work suggests that existing AI-generated image detectors are easily deceived by adversarial perturbations, highlighting the need for more robust detectors.}
}

@article{yu2025confitv2improvingresumejob,
  title={ConFit v2: Improving Resume-Job Matching using Hypothetical Resume Embedding and Runner-Up Hard-Negative Mining},
  author={Yu, Xiao * and Xu, Ruize * and Xue, Chengyuan * and Zhang, Jinzhong and Ma, Xu and Yu, Zhou},
  journal={<span style="color: #0088cc; font-style: normal">ACL 2025.</span> Findings of the Association for Computational Linguistics (ACL)},
  year={2025},
  bibbase_note={(* Equal contribution)},
  url_Paper = {https://arxiv.org/abs/2502.12361},
  abstract ={A reliable resume-job matching system helps a company recommend suitable candidates from a pool of resumes and helps a job seeker find relevant jobs from a list of job posts. However, since job seekers apply only to a few jobs, interaction labels in resume-job datasets are sparse. We introduce ConFit v2, an improvement over ConFit to tackle this sparsity problem. We propose two techniques to enhance the encoder's contrastive training process: augmenting job data with hypothetical reference resume generated by a large language model; and creating high-quality hard negatives from unlabeled resume/job pairs using a novel hard-negative mining strategy. We evaluate ConFit v2 on two real-world datasets and demonstrate that it outperforms ConFit and prior methods (including BM25 and OpenAI text-embedding-003), achieving an average absolute improvement of 13.8% in recall and 17.5% in nDCG across job-ranking and resume-ranking tasks.}
}